# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qg2-DUDLW4O1BqaodoXstuoKX0KU4KUW
"""

###############################################################################
# Library Installations
###############################################################################
# For installing PyTorch and other components
# This code block is left as a comment.
# You can install packages manually with pip:
!pip install torch==2.3.0+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install transformers datasets tensorboard ninja

###############################################################################
# Downloading zip file from Google Drive
###############################################################################
def download_file(url, destination):
    if os.path.exists(destination):
        print(f"File already exists: {destination}")
        return True

    try:
        print(f"Downloading file from {url}...")
        response = requests.get(url, stream=True)
        response.raise_for_status()

        total_size_in_bytes = int(response.headers.get('content-length', 0))
        block_size = 8192
        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)

        with open(destination, 'wb') as file:
            for data in response.iter_content(block_size):
                progress_bar.update(len(data))
                file.write(data)
        progress_bar.close()

        print(f"Download complete: {destination}")
        return True
    except Exception as e:
        print(f"Download error: {e}")
        return False

###############################################################################
# Connecting to Google Drive and retrieving the file
###############################################################################
# Google Drive file download URL (should be replaced with the actual URL)
drive_url = "YOUR_GOOGLE_DRIVE_DIRECT_DOWNLOAD_LINK_HERE"
data_dir = "./"
zip_path = os.path.join(data_dir, "03_numerized_data.zip")

# Target directory check and creation
target_dir = os.path.dirname(zip_path)
if not os.path.exists(target_dir) and target_dir:
    os.makedirs(target_dir)
    print(f"{target_dir} folder created.")

# If you are running in Colab, you can use this code
try:
    from google.colab import drive
    drive.mount('/content/drive')
    print("Google Drive connected. You can copy the file from Drive.")
    # File path in Google Drive (should be changed)
    drive_zip_path = "/content/drive/MyDrive/03_numerized_data.zip"
    if os.path.exists(drive_zip_path):
        import shutil
        shutil.copy(drive_zip_path, zip_path)
        print(f"File copied: {drive_zip_path} -> {zip_path}")
    else:
        print(f"File not found: {drive_zip_path}")
except:
    print("Not running in Google Colab environment. Direct download will be attempted.")
    if not os.path.exists(zip_path):
        download_file(drive_url, zip_path)
    else:
        print(f"File already exists: {zip_path}")

print(f"Process completed. File is at: {zip_path}")

###############################################################################
# Unzipping the zip file and loading data
###############################################################################
data_dir = "03_numerized_data"
os.makedirs(data_dir, exist_ok=True)

if os.path.exists(zip_path):
    try:
        with ZipFile(zip_path, 'r') as zip_ref:
            print(f"Unzipping zip file: {zip_path}")
            zip_ref.extractall(data_dir)
            print(f"Files extracted to: {data_dir}")
    except Exception as e:
        print(f"Error while unzipping zip file: {e}")
else:
    print(f"Zip file not found: {zip_path}")

###############################################################################
# Loading data and label correction
###############################################################################
data_dir = "03_numerized_data/03_numerized_data"
try:
    print("Loading data...")
    # Using mmap_mode for memory efficiency
    train_sentences = np.load(os.path.join(data_dir, "train_sentences.npy"), mmap_mode='r')
    train_labels = np.load(os.path.join(data_dir, "train_labels.npy"), mmap_mode='r')
    val_sentences = np.load(os.path.join(data_dir, "validation_sentences.npy"), mmap_mode='r')
    val_labels = np.load(os.path.join(data_dir, "validation_labels.npy"), mmap_mode='r')
    test_sentences = np.load(os.path.join(data_dir, "test_sentences.npy"), mmap_mode='r')
    test_labels = np.load(os.path.join(data_dir, "test_labels.npy"), mmap_mode='r')

    print(f"Original data dimensions:")
    print(f"Training data: {train_sentences.shape}, {train_labels.shape}")
    print(f"Validation data: {val_sentences.shape}, {val_labels.shape}")
    print(f"Test data: {test_sentences.shape}, {test_labels.shape}")
except Exception as e:
    print(f"Error while loading data: {e}")

    # Creating sample data in case of an error
    print("Creating sample data...")
    vocab_size = 102338
    seq_length = 100
    num_classes = 8  # 8 classes for NER tags (O, B-UNIVERSITE etc.)
    train_size = 72339
    val_size = 17339
    test_size = 17340

    train_sentences = np.random.randint(0, vocab_size, size=(train_size, seq_length))
    train_labels = np.random.randint(0, num_classes, size=(train_size,))
    val_sentences = np.random.randint(0, vocab_size, size=(val_size, seq_length))
    val_labels = np.random.randint(0, num_classes, size=(val_size,))
    test_sentences = np.random.randint(0, vocab_size, size=(test_size, seq_length))
    test_labels = np.random.randint(0, num_classes, size=(test_size,))

###############################################################################
# GPU and CUDA Information
###############################################################################
# Using Python code instead of the shell commands below
import subprocess
import sys

def check_gpu_info():
    try:
        # Run nvidia-smi command
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, check=True)
        print(result.stdout)
    except subprocess.CalledProcessError:
        print("nvidia-smi command could not be run. GPU information could not be retrieved.")
    except FileNotFoundError:
        print("nvidia-smi command not found. GPU information could not be retrieved.")

# Check GPU information
print("Checking GPU information...")
check_gpu_info()

###############################################################################
# GPU Memory Optimization - Start
###############################################################################
# Maximize available GPU memory
import gc
import torch

# torch.cuda.empty_cache() without caching
gc.collect()
torch.cuda.empty_cache()

# Disable deterministic optimizations for maximum performance
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False

print("GPU memory optimizations performed.")



# Check Torch version
import torch
print(f"Torch version: {torch.__version__}, CUDA active: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'None'}")
print(f"Number of GPUs: {torch.cuda.device_count() if torch.cuda.is_available() else 0}")
if torch.cuda.is_available():
    print(f"GPU name: {torch.cuda.get_device_name(0)}")
    print(f"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

###############################################################################
# Loading necessary libraries
###############################################################################
import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
from zipfile import ZipFile
import requests
from tqdm import tqdm
# Added for performance improvements
from torch.cuda.amp import autocast, GradScaler
import torch.nn.functional as F

###############################################################################
# Downloading zip file from Google Drive
###############################################################################
def download_file(url, destination):
    if os.path.exists(destination):
        print(f"File already exists: {destination}")
        return True

    try:
        print(f"Downloading file from {url}...")
        response = requests.get(url, stream=True)
        response.raise_for_status()

        total_size_in_bytes = int(response.headers.get('content-length', 0))
        block_size = 8192
        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)

        with open(destination, 'wb') as file:
            for data in response.iter_content(block_size):
                progress_bar.update(len(data))
                file.write(data)
        progress_bar.close()

        print(f"Download complete: {destination}")
        return True
    except Exception as e:
        print(f"Download error: {e}")
        return False

###############################################################################
# Connecting to Google Drive and retrieving the file
###############################################################################
# Google Drive file download URL (should be replaced with the actual URL)
drive_url = "YOUR_GOOGLE_DRIVE_DIRECT_DOWNLOAD_LINK_HERE"
data_dir = "./"
zip_path = os.path.join(data_dir, "03_numerized_data.zip")

# Target directory check and creation
target_dir = os.path.dirname(zip_path)
if not os.path.exists(target_dir) and target_dir:
    os.makedirs(target_dir)
    print(f"{target_dir} folder created.")

# If you are running in Colab, you can use this code
try:
    from google.colab import drive
    drive.mount('/content/drive')
    print("Google Drive connected. You can copy the file from Drive.")
    # File path in Google Drive (should be changed)
    drive_zip_path = "/content/drive/MyDrive/03_numerized_data.zip"
    if os.path.exists(drive_zip_path):
        import shutil
        shutil.copy(drive_zip_path, zip_path)
        print(f"File copied: {drive_zip_path} -> {zip_path}")
    else:
        print(f"File not found: {drive_zip_path}")
except:
    print("Not running in Google Colab environment. Direct download will be attempted.")
    if not os.path.exists(zip_path):
        download_file(drive_url, zip_path)
    else:
        print(f"File already exists: {zip_path}")

print(f"Process completed. File is at: {zip_path}")

###############################################################################
# Unzipping the zip file and loading data
###############################################################################
data_dir = "03_numerized_data/"
os.makedirs(data_dir, exist_ok=True)

if os.path.exists(zip_path):
    try:
        with ZipFile(zip_path, 'r') as zip_ref:
            print(f"Unzipping zip file: {zip_path}")
            zip_ref.extractall(data_dir)
            print(f"Files extracted to: {data_dir}")
    except Exception as e:
        print(f"Error while unzipping zip file: {e}")
else:
    print(f"Zip file not found: {zip_path}")

###############################################################################
# Loading data and label correction
###############################################################################
data_dir = "03_numerized_data/03_numerized_data"
try:
    print("Loading data...")
    # Using mmap_mode for memory efficiency
    train_sentences = np.load(os.path.join(data_dir, "train_sentences.npy"), mmap_mode='r')
    train_labels = np.load(os.path.join(data_dir, "train_labels.npy"), mmap_mode='r')
    val_sentences = np.load(os.path.join(data_dir, "validation_sentences.npy"), mmap_mode='r')
    val_labels = np.load(os.path.join(data_dir, "validation_labels.npy"), mmap_mode='r')
    test_sentences = np.load(os.path.join(data_dir, "test_sentences.npy"), mmap_mode='r')
    test_labels = np.load(os.path.join(data_dir, "test_labels.npy"), mmap_mode='r')

    print(f"Original data dimensions:")
    print(f"Training data: {train_sentences.shape}, {train_labels.shape}")
    print(f"Validation data: {val_sentences.shape}, {val_labels.shape}")
    print(f"Test data: {test_sentences.shape}, {test_labels.shape}")
except Exception as e:
    print(f"Error while loading data: {e}")

    # Creating sample data in case of an error
    print("Creating sample data...")
    vocab_size = 102338
    seq_length = 100
    num_classes = 8  # 8 classes for NER tags (O, B-UNIVERSITE etc.)
    train_size = 72339
    val_size = 17339
    test_size = 17340

    train_sentences = np.random.randint(0, vocab_size, size=(train_size, seq_length))
    train_labels = np.random.randint(0, num_classes, size=(train_size,))
    val_sentences = np.random.randint(0, vocab_size, size=(val_size, seq_length))
    val_labels = np.random.randint(0, num_classes, size=(val_size,))
    test_sentences = np.random.randint(0, vocab_size, size=(test_size, seq_length))
    test_labels = np.random.randint(0, num_classes, size=(test_size,))

###############################################################################
# Label and class count check
###############################################################################
print("Checking labels...")

# Number of classes for NER tags is fixed (taken from combined_labels.json)
# O: 0, B-UNIVERSITY: 1, B-DEPARTMENT: 2, I-UNIVERSITY: 3, I-DEPARTMENT: 4, B-NUMBER: 5, B-NAME: 6, B-SURNAME: 7
num_classes = 8

# First, ensure labels are one-dimensional
if len(train_labels.shape) > 1:
    print(f"Training labels are multi-dimensional: {train_labels.shape}, reducing to one dimension")
    if train_labels.shape[1] == 1:
        train_labels = train_labels.reshape(-1)
    else:
        print(f"Multiple columns detected, using the first column")
        train_labels = train_labels[:, 0]

if len(val_labels.shape) > 1:
    print(f"Validation labels are multi-dimensional: {val_labels.shape}, reducing to one dimension")
    if val_labels.shape[1] == 1:
        val_labels = val_labels.reshape(-1)
    else:
        val_labels = val_labels[:, 0]

if len(test_labels.shape) > 1:
    print(f"Test labels are multi-dimensional: {test_labels.shape}, reducing to one dimension")
    if test_labels.shape[1] == 1:
        test_labels = test_labels.reshape(-1)
    else:
        test_labels = test_labels[:, 0]

# Check labels - should be between 0 and 7
print("Checking NER Label values...")
all_labels = np.concatenate([train_labels, val_labels, test_labels])
unique_labels = np.unique(all_labels)
print(f"Current unique labels: {unique_labels}")

# Warn if there are issues with labels (negative or too large values)
if np.any(unique_labels < 0) or np.any(unique_labels >= num_classes):
    print(f"WARNING: Invalid label values detected! Labels should be between 0-{num_classes-1}.")
    # Clip labels to the valid range
    train_labels = np.clip(train_labels, 0, num_classes-1)
    val_labels = np.clip(val_labels, 0, num_classes-1)
    test_labels = np.clip(test_labels, 0, num_classes-1)
    print("Labels corrected and clipped to the valid range (0-7)")

# If labels are still text or a different type, convert to int64
train_labels = train_labels.astype(np.int64)
val_labels = val_labels.astype(np.int64)
test_labels = test_labels.astype(np.int64)

# Check for tensor dimension mismatch
if train_sentences.shape[0] != train_labels.shape[0]:
    print(f"WARNING: Training data dimensions do not match! Sentences: {train_sentences.shape[0]}, Labels: {train_labels.shape[0]}")
    min_train_size = min(train_sentences.shape[0], train_labels.shape[0])
    print(f"Clipping both tensors to {min_train_size} samples")
    train_sentences = train_sentences[:min_train_size]
    train_labels = train_labels[:min_train_size]

if val_sentences.shape[0] != val_labels.shape[0]:
    print(f"WARNING: Validation data dimensions do not match! Sentences: {val_sentences.shape[0]}, Labels: {val_labels.shape[0]}")
    min_val_size = min(val_sentences.shape[0], val_labels.shape[0])
    print(f"Clipping both tensors to {min_val_size} samples")
    val_sentences = val_sentences[:min_val_size]
    val_labels = val_labels[:min_val_size]

if test_sentences.shape[0] != test_labels.shape[0]:
    print(f"WARNING: Test data dimensions do not match! Sentences: {test_sentences.shape[0]}, Labels: {test_labels.shape[0]}")
    min_test_size = min(test_sentences.shape[0], test_labels.shape[0])
    print(f"Clipping both tensors to {min_test_size} samples")
    test_sentences = test_sentences[:min_test_size]
    test_labels = test_labels[:min_test_size]

print(f"Corrected data dimensions:")
print(f"Training: {train_sentences.shape}, {train_labels.shape}, Label range: {train_labels.min()}-{train_labels.max()}")
print(f"Validation: {val_sentences.shape}, {val_labels.shape}, Label range: {val_labels.min()}-{val_labels.max()}")
print(f"Test: {test_sentences.shape}, {test_labels.shape}, Label range: {test_labels.min()}-{test_labels.max()}")

###############################################################################
# Data preparation and setting model parameters
###############################################################################
# Data type selection for GPU memory limitations
# dtype = torch.float16  # Half precision to save GPU memory
# NOTE: Removing explicit dtype specification for mixed-precision training
# Autocast will already perform model computations in float16

# Converting data to PyTorch tensors and dimension correction
print("Creating tensors...")
train_sentences_tensor = torch.tensor(train_sentences, dtype=torch.long)
train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)
val_sentences_tensor = torch.tensor(val_sentences, dtype=torch.long)
val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)
test_sentences_tensor = torch.tensor(test_sentences, dtype=torch.long)
test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)

# Check label correctness
print(f"train_sentences_tensor: {train_sentences_tensor.shape}, train_labels_tensor: {train_labels_tensor.shape}, value range: {train_labels_tensor.min().item()}-{train_labels_tensor.max().item()}")
print(f"val_sentences_tensor: {val_sentences_tensor.shape}, val_labels_tensor: {val_labels_tensor.shape}, value range: {val_labels_tensor.min().item()}-{val_labels_tensor.max().item()}")
print(f"test_sentences_tensor: {test_sentences_tensor.shape}, test_labels_tensor: {test_labels_tensor.shape}, value range: {test_labels_tensor.min().item()}-{test_labels_tensor.max().item()}")

# Final check - labels must be between 0 and num_classes-1
if train_labels_tensor.max().item() >= num_classes or train_labels_tensor.min().item() < 0:
    raise ValueError(f"Training labels are in an invalid range! Should be between 0-{num_classes-1}, but are in range {train_labels_tensor.min().item()}-{train_labels_tensor.max().item()}")

# DataLoader configuration optimized for low GPU memory
batch_size = 600  # or reduce to 256
num_workers = 2  # Adjusted according to CPU core count
gradient_accumulation_steps = 16  # Gradient accumulation for larger effective batch size

train_dataset = TensorDataset(train_sentences_tensor, train_labels_tensor)
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=num_workers,
    pin_memory=True,
    prefetch_factor=2,
    persistent_workers=True
)

val_dataset = TensorDataset(val_sentences_tensor, val_labels_tensor)
val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size * 2,
    num_workers=num_workers,
    pin_memory=True,
    persistent_workers=True
)

test_dataset = TensorDataset(test_sentences_tensor, test_labels_tensor)
test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size * 2,
    num_workers=num_workers,
    pin_memory=True,
    persistent_workers=True
)

###############################################################################
# Advanced Transformer Model Class - Optimization for NER
###############################################################################
class TransformerModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, num_classes, dropout=0.1):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)

        # Positional encoding added
        self.pos_encoder = nn.Embedding(2000, embed_dim) # Max sequence length assumption

        # Normalization layer - normalizes inputs
        self.embed_norm = nn.LayerNorm(embed_dim)

        # Transformer Encoder with Advanced Layers
        encoder_layers = []
        for i in range(num_layers):
            # Pre-LayerNorm architecture (more stable training)
            encoder_layer = nn.TransformerEncoderLayer(
                d_model=embed_dim,
                nhead=num_heads,
                dim_feedforward=hidden_dim,
                dropout=dropout,
                batch_first=True,
                activation='gelu',
                norm_first=True  # Pre-LayerNorm - provides more stable training
            )
            encoder_layers.append(encoder_layer)

        self.transformer_layers = nn.ModuleList(encoder_layers)

        # To get the output of the final transformer layer
        self.final_norm = nn.LayerNorm(embed_dim)

        # Variable Output Layer (improved generalization)
        self.classifier = nn.Sequential(
            nn.Linear(embed_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )

        # Attention mechanism for token position weighting
        self.token_attention = nn.Sequential(
            nn.Linear(embed_dim, 1),
            nn.Sigmoid()
        )

        # Factor regulating gradient flow
        self.grad_multiplier = 4.0


    def forward(self, x):
        # Padding mask - mask 0-valued tokens
        padding_mask = (x == 0)

        # Embedding operation
        x_embed = self.embedding(x)

        # Add positional encoding
        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0).expand(x.size(0), -1)
        pos_embed = self.pos_encoder(positions)

        # Combine embedding and positional information
        x = x_embed + pos_embed

        # Normalize
        x = self.embed_norm(x)

        # Apply each transformer layer sequentially
        for layer in self.transformer_layers:
            # To control gradient flow
            layer_input = x
            x = layer(x, src_key_padding_mask=padding_mask)
            # Strengthen residual connection gradients
            x = x + self.grad_multiplier * 0.1 * layer_input # Scaled residual

        # Final normalize
        x = self.final_norm(x)

        # Calculate token importance
        token_weights = self.token_attention(x)

        # Weighted average for only valid (non-padding) tokens
        mask = ~padding_mask.unsqueeze(-1)  # [batch_size, seq_len, 1]
        weighted_x = x * token_weights * mask.float()
        token_sum = weighted_x.sum(dim=1)
        token_weights_sum = (token_weights * mask.float()).sum(dim=1).clamp(min=1e-9) # Avoid division by zero
        x_weighted = token_sum / token_weights_sum

        # Classification
        logits = self.classifier(x_weighted)

        return logits

###############################################################################
# Model parameters - Improved Transformer
###############################################################################
vocab_size = max(np.max(train_sentences), np.max(val_sentences), np.max(test_sentences)) + 1
embed_dim = 1024      # Reduced from 1024 to 768 (example change, kept original)
num_heads = 16       # Reduced from 16 to 12 (example change, kept original)
hidden_dim = 4096    # Reduced from 4096 to 3072 (example change, kept original)
num_layers = 8       # Reduced from 8 to 6 (example change, kept original)
dropout = 0.25       # Stronger regularization
weight_decay = 0.05  # High value for L2 regularization

print(f"Vocabulary size: {vocab_size}")
print(f"Number of classes for NER: {num_classes}")
print(f"Model parameters: embed_dim={embed_dim}, num_heads={num_heads}, hidden_dim={hidden_dim}, num_layers={num_layers}")

# Critical information - for Loss function
print(f"Class range validation for CrossEntropyLoss: Number of classes={num_classes}, Label range={train_labels_tensor.min().item()}-{train_labels_tensor.max().item()}")
if train_labels_tensor.min().item() < 0 or train_labels_tensor.max().item() >= num_classes:
    raise ValueError("Labels are in an invalid range! Do not proceed without correcting this error.")
else:
    print("✅ Label range validated, suitable for CrossEntropyLoss (NER labels between 0-7)")


###############################################################################
# GPU model optimization
###############################################################################
# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Model creation
model = TransformerModel(vocab_size, embed_dim, num_heads, hidden_dim, num_layers, num_classes, dropout)
model = model.to(device)  # Move model to GPU
print(f"Number of model parameters: {sum(p.numel() for p in model.parameters()):,}")

# Show GPU usage information
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU memory (total): {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    print(f"GPU memory (allocated): {torch.cuda.memory_allocated() / 1e9:.2f} GB")
    print(f"GPU memory (cached): {torch.cuda.memory_reserved() / 1e9:.2f} GB")


# Scaler for mixed-precision training
scaler = GradScaler(enabled=True, growth_factor=1.5, backoff_factor=0.5)


# Loss function and optimizer - For improved NER performance
# Using weighted loss to address label imbalance
# In NER, the 'O' tag is often much more frequent than others
class_weights = torch.ones(num_classes, device=device)
class_weights[0] = 0.5  # Reduce weight for 'O' tag (most common class)
for i in range(1, num_classes): # For other tags
    class_weights[i] = 2.0  # Increase weight for other tags

criterion = nn.CrossEntropyLoss(
    weight=class_weights,
    label_smoothing=0.15 # More smoothing
)

# AdamW optimizer - advanced weight decay and learning rate adjustment
optimizer = optim.AdamW(
    model.parameters(),
    lr=5e-4,  # Initial learning rate
    betas=(0.9, 0.999),
    eps=1e-8,
    weight_decay=weight_decay # Stronger regularization
)
epochs = 80 # Updated from 25 to 50 (original comment: 25'ten 50'ye güncellendi)

# Advanced learning rate scheduler - warmup and cosine decay
from torch.optim.lr_scheduler import OneCycleLR
total_steps = (len(train_loader) // gradient_accumulation_steps + (1 if len(train_loader) % gradient_accumulation_steps != 0 else 0)) * epochs
scheduler = OneCycleLR(
    optimizer,
    max_lr=2e-3, # Higher maximum learning rate
    total_steps=total_steps,
    pct_start=0.1, # 10% warmup
    anneal_strategy='cos',
    div_factor=20, # Ratio between initial and max learning rates
    final_div_factor=200 # Final learning rate factor
)


###############################################################################
# Model training
###############################################################################
epochs = 80 # Updated from 25 to 50 (original comment: 25'ten 50'ye güncellendi)
train_losses = []
train_accuracies = []
val_losses = []
val_accuracies = []
best_val_loss = float('inf')
best_val_acc = 0.0  # Also track the best validation accuracy
best_model_path = "best_model.pt"
best_model_acc_path = "best_model_acc.pt" # Best model based on accuracy
patience = 10  # Patience parameter for early stopping
patience_counter = 0  # Patience counter
start_epoch = 0 # Starting epoch, will change when resuming from checkpoint
checkpoint_path = "model_checkpoint_latest.pt" # To store the latest checkpoint

# Check to resume from checkpoint
def try_load_checkpoint():
    global model, optimizer, scheduler, start_epoch, best_val_loss, best_val_acc, train_losses, train_accuracies, val_losses, val_accuracies

    # First check the latest checkpoint
    if os.path.exists(checkpoint_path):
        print(f"Latest checkpoint found: {checkpoint_path}")
        try:
            checkpoint = torch.load(checkpoint_path, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

            # Load scheduler state if it exists
            if 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict'] is not None:
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

            start_epoch = checkpoint['epoch'] + 1  # Start from the epoch after the last saved one

            if 'best_val_loss' in checkpoint:
                best_val_loss = checkpoint['best_val_loss']
            if 'best_val_acc' in checkpoint:
                best_val_acc = checkpoint['best_val_acc']
            if 'train_losses' in checkpoint:
                train_losses = checkpoint['train_losses']
            if 'train_accuracies' in checkpoint:
                train_accuracies = checkpoint['train_accuracies']
            if 'val_losses' in checkpoint:
                val_losses = checkpoint['val_losses']
            if 'val_accuracies' in checkpoint:
                val_accuracies = checkpoint['val_accuracies']


            print(f"Resuming training from epoch {start_epoch}/{epochs}")
            print(f"Best validation loss: {best_val_loss:.4f}, Best validation accuracy: {best_val_acc:.2f}%")
            return True
        except Exception as e:
            print(f"Error loading checkpoint: {e}")
            return False


    # If latest checkpoint doesn't exist, check for a specific epoch checkpoint
    latest_epoch = -1
    latest_checkpoint_file = None # Renamed from latest_checkpoint to avoid conflict

    # Check all checkpoint files
    for filename in os.listdir('.'):
        if filename.startswith("model_checkpoint_epoch_"):
            try:
                epoch_num = int(filename.split('_')[-1].split('.')[0])
                if epoch_num > latest_epoch:
                    latest_epoch = epoch_num
                    latest_checkpoint_file = filename
            except:
                continue

    if latest_checkpoint_file: # Check if a file was found
        print(f"Latest epoch checkpoint found: {latest_checkpoint_file}")
        try:
            checkpoint = torch.load(latest_checkpoint_file, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

            # Load scheduler state if it exists
            if 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict'] is not None:
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])


            start_epoch = checkpoint['epoch'] + 1 # Start from the epoch after the last saved one

            print(f"Resuming training from epoch {start_epoch}/{epochs}")
            return True
        except Exception as e:
            print(f"Error loading checkpoint: {e}")
            return False

    print("Checkpoint not found. Training will start from scratch.")
    return False


###############################################################################
# Optimized training functions
###############################################################################
# Training function (with mixed precision and gradient accumulation)
def train_epoch(model, loader, optimizer, criterion, device, scaler, scheduler):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    optimizer.zero_grad(set_to_none=True) # More efficient zeroing

    progress_bar = tqdm(loader, desc="Training")
    for batch_idx, (inputs, targets) in enumerate(progress_bar):
        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)

        # Mixed precision training
        with autocast(): # `enabled=True` is default if scaler is enabled
            outputs = model(inputs)
            loss = criterion(outputs, targets) / gradient_accumulation_steps # Scale loss

        # Mixed precision backpropagation
        scaler.scale(loss).backward()

        # Gradient accumulation - optimize every N steps
        if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(loader):
            # Prevent gradient explosion
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True) # Zero gradients after optimizer step

            # OneCycleLR is updated only on actual optimizer steps
            scheduler.step()

            # Print learning rates at regular intervals
            if batch_idx % (gradient_accumulation_steps * 10) == 0: # Check more frequently
                lr = optimizer.param_groups[0]['lr']
                print(f"  Batch: {batch_idx}/{len(loader)}, LR: {lr:.6f}")


        # Calculate actual loss (unscaled)
        total_loss += loss.item() * gradient_accumulation_steps # Accumulate unscaled loss
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

        # Progress bar update
        progress_bar.set_postfix({
            'loss': f"{total_loss/(batch_idx+1):.4f}", # Average loss per batch so far
            'acc': f"{100.*correct/total:.2f}%",
            'lr': f"{scheduler.get_last_lr()[0]:.6f}"
        })

        # Clear GPU cache
        if batch_idx % 5 == 0: # Clear more frequently
            torch.cuda.empty_cache()


    accuracy = 100. * correct / total
    avg_loss = total_loss / len(loader) # Average loss over all batches
    return avg_loss, accuracy

# Validation function
def validate(model, loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        progress_bar = tqdm(loader, desc="Validation")
        for batch_idx, (inputs, targets) in enumerate(progress_bar):
            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)

            with autocast(): # Use autocast for validation too
                outputs = model(inputs)
                loss = criterion(outputs, targets)

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            # Progress bar update
            progress_bar.set_postfix({
                'loss': f"{total_loss/(batch_idx+1):.4f}",
                'acc': f"{100.*correct/total:.2f}%"
            })

    accuracy = 100. * correct / total
    avg_loss = total_loss / len(loader)
    return avg_loss, accuracy


###############################################################################
# Model training
###############################################################################
print("Model training starting...")

try:
    # Try to resume from checkpoint
    try_load_checkpoint()

    for epoch in range(start_epoch, epochs):
        print(f"\nStarting Epoch {epoch+1}/{epochs}...")

        # Clear GPU cache
        torch.cuda.empty_cache()

        # Training
        train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, criterion, device, scaler, scheduler)
        train_losses.append(train_loss)
        train_accuracies.append(train_accuracy)

        # Clear GPU cache before validation
        torch.cuda.empty_cache()

        # Validation
        val_loss, val_accuracy = validate(model, val_loader, criterion, device)
        val_losses.append(val_loss)
        val_accuracies.append(val_accuracy)

        # Get current learning rate
        current_lr = optimizer.param_groups[0]['lr']

        # Save the best model - loss-based
        model_improved = False # Not used later, can be removed
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            # model_improved = True # This assignment is not used

            # Save model state and configuration
            checkpoint_content = { # Renamed from checkpoint to avoid conflict
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict() if hasattr(scheduler, 'state_dict') else None,
                'loss': val_loss,
                'accuracy': val_accuracy,
                'config': {
                    'vocab_size': vocab_size,
                    'embed_dim': embed_dim,
                    'num_heads': num_heads,
                    'hidden_dim': hidden_dim,
                    'num_layers': num_layers,
                    'num_classes': num_classes,
                    'dropout': dropout
                }
            }

            torch.save(checkpoint_content, best_model_path)
            print(f"New best model saved (loss): {best_model_path}")
            patience_counter = 0  # Reset patience counter
        else:
            patience_counter += 1  # Increment patience counter

        # Also save the best model based on accuracy
        if val_accuracy > best_val_acc:
            best_val_acc = val_accuracy

            # Save accuracy-based model state
            acc_checkpoint_content = { # Renamed from acc_checkpoint
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'accuracy': val_accuracy,
                'loss': val_loss, # Also save loss for reference
                'config': {
                    'vocab_size': vocab_size,
                    'embed_dim': embed_dim,
                    'num_heads': num_heads,
                    'hidden_dim': hidden_dim,
                    'num_layers': num_layers,
                    'num_classes': num_classes,
                    'dropout': dropout
                }
            }

            torch.save(acc_checkpoint_content, best_model_acc_path)
            print(f"New best model saved (acc): {best_model_acc_path}")


        print(f"Epoch {epoch+1}/{epochs}: "
              f"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, "
              f"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, "
              f"LR: {current_lr:.6f}")

        # Save the latest state at the end of each epoch (latest checkpoint)
        latest_checkpoint_content = { # Renamed from latest_checkpoint
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if hasattr(scheduler, 'state_dict') else None,
            'loss': val_loss,
            'accuracy': val_accuracy,
            'best_val_loss': best_val_loss,
            'best_val_acc': best_val_acc,
            'train_losses': train_losses,
            'train_accuracies': train_accuracies,
            'val_losses': val_losses,
            'val_accuracies': val_accuracies
        }
        torch.save(latest_checkpoint_content, checkpoint_path)
        print(f"Latest state saved: {checkpoint_path}")


        # Save intermediate checkpoint every 5 epochs
        if (epoch + 1) % 5 == 0:
            specific_checkpoint_path = f"model_checkpoint_epoch_{epoch+1}.pt"
            torch.save(latest_checkpoint_content, specific_checkpoint_path) # Save the latest_checkpoint_content
            print(f"Checkpoint saved: {specific_checkpoint_path}")


        # GPU usage statistics
        if torch.cuda.is_available():
            print(f"GPU memory (allocated): {torch.cuda.memory_allocated() / 1e9:.2f} GB")
            print(f"GPU memory (cached): {torch.cuda.memory_reserved() / 1e9:.2f} GB")


        # Early stopping check
        if patience_counter >= patience:
            print(f"Validation loss has not improved for {patience} epochs. Training stopped early.")
            print(f"Best validation loss: {best_val_loss:.4f}, Best validation accuracy: {best_val_acc:.2f}%")
            break

except KeyboardInterrupt:
    print("Training interrupted by user.")

    # Save current state in case of interruption
    # Need to define `epoch` if loop didn't start, or handle this carefully
    # Assuming `epoch` is defined from the loop
    if 'epoch' not in locals(): # If interrupted before first epoch completed
        epoch = start_epoch -1 # Or some other placeholder

    interrupt_checkpoint_content = { # Renamed from interrupt_checkpoint
        'epoch': epoch, # This might be -1 if interrupted before first epoch starts
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if hasattr(scheduler, 'state_dict') else None,
        'best_val_loss': best_val_loss,
        'best_val_acc': best_val_acc
        # Consider also saving train/val losses/accuracies arrays if they exist
    }
    torch.save(interrupt_checkpoint_content, "model_interrupt.pt")
    print("Interruption state saved: model_interrupt.pt")


except Exception as e:
    print(f"Error during training: {e}")
    import traceback
    traceback.print_exc()


import torch # Already imported, but good for clarity
torch.cuda.empty_cache()
torch.cuda.ipc_collect() # For more thorough cleanup if needed

###############################################################################
# Test evaluation
###############################################################################
# Clear GPU cache
torch.cuda.empty_cache()

# Load the best model
if os.path.exists(best_model_path):
    checkpoint_data = torch.load(best_model_path, map_location=device) # Renamed from checkpoint

    # Recreate the model with new parameters
    config = checkpoint_data['config']
    # Ensure model is re-instantiated before loading state_dict
    model = TransformerModel(
        config['vocab_size'],
        config['embed_dim'],
        config['num_heads'],
        config['hidden_dim'],
        config['num_layers'],
        config['num_classes'],
        config['dropout']
    )

    model.load_state_dict(checkpoint_data['model_state_dict'])
    model = model.to(device)  # Move model to GPU, without specifying dtype
    print(f"Best model loaded: Epoch {checkpoint_data['epoch']+1}, Accuracy: {checkpoint_data.get('accuracy', 'N/A'):.2f}%") # Use .get for safety
else:
    print("Best model not found, current model will be used")

# Test evaluation
test_loss, test_accuracy = validate(model, test_loader, criterion, device)
print(f"Test Accuracy: {test_accuracy:.2f}%")
print(f"Test Loss: {test_loss:.4f}")

###############################################################################
# Plotting training graphs
###############################################################################
plt.figure(figsize=(12, 5))

# Loss graph
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Training Loss', color='blue')
plt.plot(val_losses, label='Validation Loss', color='red')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)

# Accuracy graph
plt.subplot(1, 2, 2)
plt.plot(train_accuracies, label='Training Accuracy', color='blue')
plt.plot(val_accuracies, label='Validation Accuracy', color='red')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('training_graphs.png')
plt.show()

###############################################################################
# Saving to Google Drive (when running in Colab)
###############################################################################
try:
    from google.colab import drive
    if not os.path.exists('/content/drive'): # Check if already mounted
        drive.mount('/content/drive')

    # Directory to save to
    save_dir = "/content/drive/MyDrive/model_saves" # Changed from "model_kayitlari"
    os.makedirs(save_dir, exist_ok=True)

    # Copy the best model to Google Drive
    if os.path.exists(best_model_path):
        import shutil
        drive_model_path = os.path.join(save_dir, "best_model.pt")
        shutil.copy(best_model_path, drive_model_path)
        print(f"Model saved to Google Drive: {drive_model_path}")

    # Copy graphs to Google Drive
    if os.path.exists('training_graphs.png'): # Changed from "egitim_grafikleri.png"
        import shutil
        drive_graph_path = os.path.join(save_dir, "training_graphs.png")
        shutil.copy('training_graphs.png', drive_graph_path)
        print(f"Graphs saved to Google Drive: {drive_graph_path}")
except ModuleNotFoundError: # More specific exception for colab import
    print("Not running in Google Colab environment or Drive connection failed.")
except Exception as e: # Catch other potential errors
    print(f"An error occurred while saving to Google Drive: {e}")
